{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124dc383",
   "metadata": {},
   "source": [
    "# Notebook to automate LLM Inference using LangChain\n",
    "Using Langchain as a framework to interact with Ollama models beyond the Terminal along with the possibility of setting up a pipeline to import an image, run Model inference and export the output to its corresponding study folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bc4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and functions required to convert images into base64 for the model inference\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fdab7",
   "metadata": {},
   "source": [
    "### Converting dicom scans to png images for the Vision Models \n",
    "This is merely an intermediate before conversion to base64 but all images are saved as .png hence this only needs to be run once for each study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c15478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert dcm files to png\n",
    "def dicom_to_png(dicom_path, png_path):\n",
    "    ds = pydicom.dcmread(dicom_path)\n",
    "    pixel_array = ds.pixel_array.astype(float)\n",
    "    scaled = (np.maximum(pixel_array, 0) / pixel_array.max()) * 255.0\n",
    "    img = Image.fromarray(scaled.astype(np.uint8)).convert(\"RGB\")\n",
    "    img.save(png_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = './dataset'\n",
    "\n",
    "# this gives in each sub folder within dataset s0, s1, s2\n",
    "folders = sorted(os.listdir(data_dir))\n",
    "for folder in folders:\n",
    "   s_folder = os.path.join(data_dir, folder)\n",
    "   # gives all .dcm and .txt files within each study\n",
    "   all_files = os.listdir(s_folder)\n",
    "   dcm_files = [f for f in all_files if f.lower().endswith('.dcm')]\n",
    "   \n",
    "   for each_file in dcm_files:\n",
    "    path_to_save = os.path.join(s_folder, f\"{each_file[0:3]}.png\")\n",
    "    dicom_path = os.path.join(s_folder, each_file)\n",
    "    dicom_to_png(dicom_path, path_to_save)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15955c3c",
   "metadata": {},
   "source": [
    "### Converting .png images to base64 for Ollama\n",
    "Ollama Models require images to be sent through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"PNG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2dba8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0784ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which takes in data and converts into LangChain human message for the Model\n",
    "def prompt_func(data):\n",
    "    text = data[\"text\"]\n",
    "    image = data[\"image\"]\n",
    "\n",
    "    image_part = {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{image}\",\n",
    "    }\n",
    "\n",
    "    content_parts = []\n",
    "\n",
    "    text_part = {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    content_parts.append(image_part)\n",
    "    content_parts.append(text_part)\n",
    "\n",
    "    return [HumanMessage(content=content_parts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gemma3:4b-it-qat', 'qwen2.5vl:3b']\n",
    "\n",
    "prompt = \"You are a radiologist reviewing this imaging study. Based on the image provided, generate only the *Findings* section of a radiology report. Use structured, concise, and formal language consistent with professional radiology reporting. Do not include the Impression, Conclusion, or Recommendations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7d166f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatOllama(model=models[0], temperature=0)\n",
    "# chain = prompt_func | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb709387",
   "metadata": {},
   "source": [
    "## Zero Shot Prompting\n",
    "Getting the model to generate findings for Radiology Scans without seeing any prior images and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0073c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "from PIL import Image\n",
    "\n",
    "# this gives in each sub folder within dataset s0, s1, s2\n",
    "folders = sorted(os.listdir(data_dir))\n",
    "\n",
    "for folder in folders:\n",
    "    s_folder = os.path.join(data_dir, folder)\n",
    "    # gives files within each study\n",
    "    all_files = os.listdir(s_folder)\n",
    "    image_files = [f for f in all_files if f.lower().endswith('.png')]\n",
    "\n",
    "    for each_file in image_files:\n",
    "        image_path = os.path.join(s_folder, each_file)\n",
    "        # file to write output into\n",
    "        model_output_path =  os.path.join(s_folder, f\"qwen-vl-3b-{each_file[0:3]}.txt\")\n",
    "\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            # convert to base64 version of image\n",
    "            base64_str = convert_to_base64(img)\n",
    "            query_chain = chain.invoke(\n",
    "                {\"text\": prompt, \n",
    "                \"image\": base64_str}\n",
    "                )\n",
    "            # write model output into a file\n",
    "            with open(model_output_path, \"w\") as file:\n",
    "                file.write(query_chain)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on {each_file}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174b8c7",
   "metadata": {},
   "source": [
    "## Few Shot Prompting\n",
    "Giving the model prior example to see various radiology scans and then invoke inference based off of given examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f2ba6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_findings(report_text):\n",
    "    match = re.search(r'FINDINGS:(.*?)(IMPRESSION:|$)', report_text, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943a52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "examples = []\n",
    "few_shot_studies = ['s0', 's1', 's2', 's3', 's4']\n",
    "\n",
    "for folder in few_shot_studies:\n",
    "    s_folder = os.path.join(data_dir, folder)\n",
    "    # gives files within each study\n",
    "    all_files = os.listdir(s_folder)\n",
    "    image_files = [f for f in all_files if f.lower().endswith('.png')]\n",
    "\n",
    "    # this contains the single ground truth file\n",
    "    text_files = [f for f in all_files if f.lower().endswith('.txt')]\n",
    "    \n",
    "    for each_file in image_files:\n",
    "        image_path = os.path.join(s_folder, each_file)\n",
    "    \n",
    "        img = Image.open(image_path)\n",
    "        # convert to base64 version of image\n",
    "        base64_str = convert_to_base64(img)\n",
    "        f = open(os.path.join(s_folder, text_files[0]))\n",
    "        report_truth = f.read()\n",
    "\n",
    "        examples.append({\"input\": base64_str, \"output\": extract_findings(report_truth)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ecbb0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d35eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = final_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5be8a3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very long string of seemingly random characters. It appears to be a base64 encoded image. Let's decode it.\n",
      "\n",
      "Using a base64 decoding tool (I'll use an online one: [https://www.base64decode.org/](https://www.base64decode.org/)), the decoded image is:\n",
      "\n",
      "**A simple black and white image of a cat.**\n",
      "\n",
      "The cat is sitting, looking to the left. It's a very basic, almost cartoon-like representation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('./dataset/s0/02a.png')\n",
    "image = convert_to_base64(img)\n",
    "query_chain = chain.invoke({\"input\": image })\n",
    "\n",
    "print(query_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
