{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124dc383",
   "metadata": {},
   "source": [
    "# LLM Inference via Ollama API \n",
    "Running Model inference using Ollama API for few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bc4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and functions required to convert images into base64 for the model inference\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fdab7",
   "metadata": {},
   "source": [
    "### Converting dicom scans to png images for the Vision Models \n",
    "This is merely an intermediate before conversion to base64 but all images are saved as .png hence this only needs to be run once for each study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15955c3c",
   "metadata": {},
   "source": [
    "### Converting .png images to base64 for Ollama\n",
    "Ollama Models require images to be sent through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"PNG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508454ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gemma3:4b-it-qat', 'qwen2.5vl:3b']\n",
    "\n",
    "prompt = \"You are a radiologist reviewing this imaging study. Based on the image provided, generate only the *Findings* section of a radiology report. Use structured, concise, and formal language consistent with professional radiology reporting. Do not include the Impression, Conclusion, or Recommendations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4863c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./dataset/s11/1b1.png\")\n",
    "base64_img = convert_to_base64(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb709387",
   "metadata": {},
   "source": [
    "## Zero Shot Prompting\n",
    "Getting the model to generate findings for Radiology Scans without seeing any prior images and ground truth\n",
    "Currently just testing out 2-4 images to test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57058ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gemma3:4b-it-qat', 'created_at': '2025-07-19T20:13:30.0138587Z', 'response': 'Findings:\\n*   Mild increased interstitial markings are noted in both lungs.\\n*   Mild cardiomediastinal contour.\\n*   Right pneumothorax with moderate air density noted in the right hemithorax. A pleural effusion is not directly visualized.\\n*   No acute lung consolidations identified.\\n*   Right chest tube present.', 'done': True, 'done_reason': 'stop', 'context': [105, 2364, 107, 236840, 3024, 236772, 236771, 236842, 107, 107, 3048, 659, 496, 4574, 16097, 35329, 672, 20502, 2748, 236761, 18186, 580, 506, 2471, 3847, 236764, 8729, 1186, 506, 808, 65362, 236829, 3336, 529, 496, 131230, 2072, 236761, 6890, 31044, 236764, 63510, 236764, 532, 10781, 5192, 9958, 607, 5707, 131230, 13761, 236761, 3574, 711, 3204, 506, 118340, 236764, 63916, 236764, 653, 91330, 236761, 106, 107, 105, 4368, 107, 107, 65362, 236787, 107, 236829, 138, 84432, 4869, 57610, 68189, 659, 8601, 528, 1800, 38464, 236761, 107, 236829, 138, 84432, 50788, 542, 13814, 752, 1049, 34993, 236761, 107, 236829, 138, 12488, 29229, 174965, 1124, 607, 21037, 2634, 7620, 8601, 528, 506, 1447, 12147, 592, 504, 1124, 236761, 562, 181069, 156605, 563, 711, 5467, 88308, 236761, 107, 236829, 138, 2301, 20719, 17195, 27856, 847, 8385, 236761, 107, 236829, 138, 12488, 15350, 11945, 1861, 236761], 'total_duration': 62383359900, 'load_duration': 72297100, 'prompt_eval_count': 325, 'prompt_eval_duration': 48658411600, 'eval_count': 73, 'eval_duration': 13611244900}\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"model\" : \"gemma3:4b-it-qat\",\n",
    "    \"prompt\" : prompt,\n",
    "    \"stream\" : False, \n",
    "    \"images\": [base64_img]\n",
    "}\n",
    "\n",
    "ollama_url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "response = requests.post(ollama_url, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174b8c7",
   "metadata": {},
   "source": [
    "## Few Shot Prompting\n",
    "Giving the model prior example to see various radiology scans and then invoke inference based off of given examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2ba6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_findings(report_text):\n",
    "    match = re.search(r'FINDINGS:(.*?)(IMPRESSION:|$)', report_text, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6db3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt,\n",
    "      \"images\": []\n",
    "}\n",
    "\n",
    "model_message = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"\",\n",
    "    \"images\": None\n",
    "}\n",
    "\n",
    "data = {\n",
    "  \"model\": \"gemma3:4b-it-qat\",\n",
    "  \"messages\": [],\n",
    "  \"stream\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "943a52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset'\n",
    "examples = []\n",
    "few_shot_studies = ['s0', 's1', 's2']\n",
    "\n",
    "for folder in few_shot_studies:\n",
    "    s_folder = os.path.join(data_dir, folder)\n",
    "    # gives files within each study\n",
    "    all_files = os.listdir(s_folder)\n",
    "    image_files = [f for f in all_files if f.lower().endswith('.png')]\n",
    "\n",
    "    # this contains the single ground truth file\n",
    "    text_files = [f for f in all_files if f.lower().endswith('.txt')]\n",
    "    \n",
    "    for each_file in image_files:\n",
    "       \n",
    "        image_path = os.path.join(s_folder, each_file)\n",
    "        img = Image.open(image_path)\n",
    "        # convert to base64 version of image\n",
    "        base64_str = convert_to_base64(img)\n",
    "        f = open(os.path.join(s_folder, text_files[0]))\n",
    "        report_truth = extract_findings(f.read())\n",
    "\n",
    "        h_message = human_message.copy()\n",
    "        h_message[\"images\"].append(base64_str)\n",
    "        ai_message = model_message.copy()\n",
    "        ai_message[\"content\"] = report_truth\n",
    "        data[\"messages\"].append(h_message)\n",
    "        data[\"messages\"].append(ai_message)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148855e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_study = \"./dataset/s12/57c.png\"\n",
    "img = Image.open(test_study)\n",
    "base64_img = convert_to_base64(img)\n",
    "\n",
    "h_message = human_message.copy()\n",
    "h_message[\"images\"].append(base64_img)\n",
    "\n",
    "data[\"messages\"].append(h_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5108c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'model runner has unexpectedly stopped, this may be due to resource limitations or an internal error, check ollama server logs for details'}\n"
     ]
    }
   ],
   "source": [
    "ollama_url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "response = requests.post(ollama_url, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"messages\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
